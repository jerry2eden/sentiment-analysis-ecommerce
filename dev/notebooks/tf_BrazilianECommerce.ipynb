{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Project-Variables\" data-toc-modified-id=\"Project-Variables-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Project Variables</a></span></li><li><span><a href=\"#Dataset-and-Pipelines\" data-toc-modified-id=\"Dataset-and-Pipelines-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset and Pipelines</a></span></li><li><span><a href=\"#Building-the-Deep-NN\" data-toc-modified-id=\"Building-the-Deep-NN-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Building the Deep NN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Network-Variables\" data-toc-modified-id=\"Network-Variables-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Network Variables</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is building a sentiment classification model using plain TensorFlow. For making this task easier, we will use data prep steps (like pipelines and text functions) for preparing the data for feeding a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T17:03:13.930659Z",
     "start_time": "2020-09-25T17:03:05.825753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "\n",
    "# Utilities\n",
    "from utils.custom_transformers import import_data\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T15:46:40.681735Z",
     "start_time": "2020-09-25T15:46:40.668769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variables for path definition\n",
    "DATA_PATH = '../../data'\n",
    "PIPE_PATH = '../../pipelines'\n",
    "\n",
    "# Variables reading files\n",
    "DATASET_FILENAME = 'olist_order_reviews_dataset.csv'\n",
    "DATASET_COLS = ['review_comment_message', 'review_score']\n",
    "FEATURES_COL = 'review_comment_message'\n",
    "TARGET_COL = 'review_score'\n",
    "\n",
    "TEXT_PIPE = 'text_prep_pipeline.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, let's read the raw data and apply the text prep pipeline already built on python training script on `dev/training` project folder. The goal is to give the raw text as input and transform this data into features using the vectorizer implemented on the pipeline (`TfIdfVectorizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T16:51:07.662541Z",
     "start_time": "2020-09-25T16:50:51.581319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Shape of X_train data: (33402, 650)\n",
      "--------------------------------------------------\n",
      "Shape of X_test data: (8351, 650)\n",
      "\n",
      "Samples of y_train: [1 4 4 5 5]\n",
      "Samples of y_test: [1 1 3 3 1]\n"
     ]
    }
   ],
   "source": [
    "# Reading the data and dropping duplicates\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, DATASET_FILENAME), usecols=DATASET_COLS)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X = df[FEATURES_COL].values\n",
    "y = df[TARGET_COL].values\n",
    "\n",
    "# Reading the pipeline\n",
    "text_prep_pipe = load(os.path.join(PIPE_PATH, TEXT_PIPE))\n",
    "\n",
    "# Applying it to training data\n",
    "X_prep = text_prep_pipe.fit_transform(X)\n",
    "\n",
    "# Splitting into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prep, y, test_size=.20, random_state=42)\n",
    "\n",
    "# Results\n",
    "for data, name in zip([X_train, X_test], ['X_train', 'X_test']):\n",
    "    print('-' * 50)\n",
    "    print(f'Shape of {name} data: {data.shape}')\n",
    "print(f'\\nSamples of y_train: {y_train[:5]}')\n",
    "print(f'Samples of y_test: {y_test[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Deep NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rading and preparing the data for feeding it into a neural network, let's retrieve some useful parameters for the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T17:04:03.002019Z",
     "start_time": "2020-09-25T17:04:02.989024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Neural network inputs: 650\n",
      "Number of classes: 5 - Sample: [1 0 0 0 0]\n",
      "Structure:\n",
      " - 650 - 300 - 100 - 5\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieving data info\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))\n",
    "\n",
    "# Transforming the classes in one hot vectors\n",
    "y_train_oh = pd.get_dummies(y_train).values\n",
    "\n",
    "# Neural network structure\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "\n",
    "# Overview\n",
    "print('-' * 40)\n",
    "print(f'Neural network inputs: {n_inputs}')\n",
    "print(f'Number of classes: {n_classes} - Sample: {y_train_oh[0]}')\n",
    "print(f'Structure:')\n",
    "for units in n_inputs, n_hidden1, n_hidden2, n_classes:\n",
    "    print(f' - {units}', end='')\n",
    "print()\n",
    "print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T17:04:04.599142Z",
     "start_time": "2020-09-25T17:04:04.589178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Função para reset do grafo\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T17:04:05.051497Z",
     "start_time": "2020-09-25T17:04:05.040495Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T17:04:06.577534Z",
     "start_time": "2020-09-25T17:04:05.241070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A74C99EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A74C99EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A74C99EB8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A74C99EB8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A6C21ADD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A6C21ADD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A6C21ADD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A6C21ADD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A6C21ADD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A6C21ADD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A6C21ADD8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000012A6C21ADD8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# Variáveis para salvamento do modelo\n",
    "now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "root_logdir = 'tf_logs'\n",
    "logdir = f'{root_logdir}/run_{now}'\n",
    "\n",
    "# ----------------------------\n",
    "# ---- CONSTRUCTION PHASE ----\n",
    "# ----------------------------\n",
    "\n",
    "# Definindo placeholders para os inputs\n",
    "reset_graph()\n",
    "with tf.name_scope('inputs'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "    y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "    \n",
    "# Construindo as camadas da rede\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2')\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name='outputs')\n",
    "    \n",
    "# Definindo função custo\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    \n",
    "# Definindo otimizador\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "# Avaliando performance (acurácia)\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "\"\"\"# Avaliando performance (AUC)\n",
    "with tf.name_scope('auc'):\n",
    "    auc = tf.keras.metrics.AUC(y_proba, correct)\"\"\"\n",
    "    \n",
    "# Nós de inicialização e salvamento da rede\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Parâmetros para visualização no TensorBoard\n",
    "loss_summary = tf.summary.scalar('loss', loss)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T17:04:12.951971Z",
     "start_time": "2020-09-25T17:04:12.946985Z"
    }
   },
   "outputs": [],
   "source": [
    "# Definindo função para leitura de dados em mini-batches\n",
    "def fetch_batch(X, y, epoch, batch_index, batch_size):\n",
    "    \"\"\"\n",
    "    Etapas:\n",
    "        1. leitura do conjunto de dados em diferentes mini-batches\n",
    "        \n",
    "    Argumentos:\n",
    "        epoch -- época do treinamento do algoritmo\n",
    "        batch_index -- índice do mini-batch a ser lido do conjunto total\n",
    "        batch_size -- tamanho do mini-batch em termos de número de registros\n",
    "        \n",
    "    Retorno:\n",
    "        X_batch, y_batch -- conjuntos mini-batch de dados lidos a partir do conjunto total\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retornando parâmetros\n",
    "    m = X.shape[0]\n",
    "    n_batches = m // batch_size\n",
    "    \n",
    "    # Definindo semente aleatória\n",
    "    np.random.seed(epoch * n_batches + batch_index)\n",
    "    \n",
    "    # Indexando mini-batches do conjunto total\n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = X[indices]\n",
    "    y_batch = y[indices]\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-25T17:04:32.432656Z",
     "start_time": "2020-09-25T17:04:32.283805Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-7700e0b79323>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;31m# Salvando status do modelo a cada T mini-batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0msummary_loss_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_summary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_feed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn_batches\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mfile_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_loss_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \"\"\"\n\u001b[1;32m--> 731\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    732\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5577\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5578\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 5579\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# ---- EXECUTION PHASE ----\n",
    "# -------------------------\n",
    "\n",
    "# Variáveis importantes para o treinamento\n",
    "m_train = X_train.shape[0]\n",
    "n_epochs = 50\n",
    "batch_size = 128\n",
    "n_batches = m_train // batch_size\n",
    "costs = []\n",
    "\n",
    "# Inicializando sessão\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Inicializando variáveis globais\n",
    "    init.run()\n",
    "    \n",
    "    # Iterando sobre as épocas de treino\n",
    "    for epoch in range(n_epochs):\n",
    "        # Iterando sobre cada mini-batch\n",
    "        for batch in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(X_train, y_train, epoch, batch, batch_size)\n",
    "            batch_feed_dict = {X: X_batch, y: y_batch}\n",
    "            \n",
    "            # Salvando status do modelo a cada T mini-batches\n",
    "            if batch % 10 == 0:\n",
    "                summary_loss_str = loss_summary.eval(feed_dict=batch_feed_dict)\n",
    "                step = epoch * n_batches + batch\n",
    "                file_writer.add_summary(summary_loss_str, step)\n",
    "                \n",
    "            # Inicializando treinamento com cada mini-batch\n",
    "            sess.run(training_op, feed_dict=batch_feed_dict)\n",
    "            \n",
    "        # Métricas de performance a cada N épocas\n",
    "        test_feed_dict = {X: X_test_prep, y: y_test}\n",
    "        if epoch % 5 == 0:\n",
    "            # Acurácia\n",
    "            acc_train = accuracy.eval(feed_dict=batch_feed_dict)\n",
    "            acc_test = accuracy.eval(feed_dict=test_feed_dict)\n",
    "            print(f'Epoch: {epoch}, Train accuracy: {round(float(acc_train), 4)}, \\\n",
    "Test accuracy: {round(float(acc_test), 4)}')\n",
    "            \n",
    "            # AUC\n",
    "            \"\"\"train_proba = y_proba.eval(feed_dict=batch_feed_dict)\n",
    "            class_indices = np.argmax(train_proba, axis=1)\n",
    "            train_pred = np.array([[classes[class_idx]] for class_idx in class_indices], np.int32)\n",
    "            tf.local_variables_initializer().run()\n",
    "            auc_train = sess.run(auc(y_batch.reshape(-1, 1), train_pred))\n",
    "            print(f'AUC: {auc_train}')\"\"\"\n",
    "            \n",
    "        # Custo do modelo\n",
    "        cost = loss.eval(feed_dict=batch_feed_dict)\n",
    "        costs.append(cost)\n",
    "        \n",
    "    # Finalizando FileWriter\n",
    "    file_writer.close()\n",
    "    \n",
    "# Plotando custo\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(np.squeeze(costs), color='navy')\n",
    "format_spines(ax, right_border=False)\n",
    "ax.set_title('Neural Network Cost', color='dimgrey')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
